{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75bb5913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict, List, Dict\n",
    "from dotenv import load_dotenv  \n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage, SystemMessage\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.graph import StateGraph, END, START\n",
    "from langgraph.prebuilt import ToolNode\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0630d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]= GROQ_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a608b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cc86cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    slide_no: int\n",
    "    subtopic: List[str]\n",
    "    content_to_display: str\n",
    "    narration_script: str\n",
    "    is_blank_slide: bool\n",
    "    image_address: str\n",
    "    video_address: str\n",
    "    image_position: str\n",
    "    content_position: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f26ed3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "PEXELS_API_KEY = 'mIZdthiPsT6hrIGHcGTOgwH61Q4UvepeUP3o9sU9GUqXm1HVhqas1fQQ'\n",
    "\n",
    "def fetch_relevant_image(query: str) -> str:\n",
    "    headers = {\n",
    "        \"Authorization\": PEXELS_API_KEY\n",
    "    }\n",
    "    params = {\n",
    "        \"query\": query,\n",
    "        \"per_page\": 1\n",
    "    }\n",
    "    response = requests.get('https://api.pexels.com/v1/search', headers=headers, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if data['photos']:\n",
    "            return data['photos'][0]['src']['medium']\n",
    "    return \"\"  # fallback if no image found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6aac11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_slide_with_media(state: AgentState) -> AgentState:\n",
    "    subtopic_text = \", \".join(state[\"subtopic\"]) if isinstance(state[\"subtopic\"], list) else state[\"subtopic\"]\n",
    "    content_text = state[\"content_to_display\"]\n",
    "\n",
    "    # Step 1: Use LLM to generate a more accurate image search query\n",
    "    query_prompt = (\n",
    "        f\"Given the subtopic: '{subtopic_text}' and content: '{content_text}', \"\n",
    "        f\"generate a short and precise search query that would retrieve the most relevant image for this content from an image search API. \"\n",
    "        f\"The query should include important keywords only, without any extra words, punctuation, or formatting. \"\n",
    "        f\"Output only the query, no explanation.\"\n",
    "    )\n",
    "\n",
    "    query_response = llm.invoke(query_prompt)\n",
    "    refined_query = query_response.content.strip()\n",
    "\n",
    "    # Step 2: Fetch image from Pexels using the refined query\n",
    "    image_address = fetch_relevant_image(refined_query)\n",
    "\n",
    "    # Step 3: Ask LLM for positioning of image and content\n",
    "    position_prompt = (\n",
    "        f\"For the subtopic '{subtopic_text}', given the content:\\n\\n\"\n",
    "        f\"Content: {content_text}\\n\\n\"\n",
    "        f\"Decide where to place the image and content on the slide.\\n\"\n",
    "        f\"Respond strictly in this format:\\n\"\n",
    "        f\"Image Position: <left/right/top/bottom>\\n\"\n",
    "        f\"Content Position: <left/right/top/bottom>\\n\"\n",
    "        f\"Choose positions that enhance readability and visual appeal.\"\n",
    "    )\n",
    "\n",
    "    position_response = llm.invoke(position_prompt)\n",
    "    output = position_response.content.strip()\n",
    "\n",
    "    image_position = \"\"\n",
    "    content_position = \"\"\n",
    "\n",
    "    for line in output.split('\\n'):\n",
    "        if line.lower().startswith(\"image position:\"):\n",
    "            image_position = line.split(\":\", 1)[1].strip()\n",
    "        elif line.lower().startswith(\"content position:\"):\n",
    "            content_position = line.split(\":\", 1)[1].strip()\n",
    "\n",
    "    # Step 4: Update state\n",
    "    state.update({\n",
    "        \"image_address\": image_address,\n",
    "        \"video_address\": \"\",  # Optional: you can implement video retrieval similarly\n",
    "        \"image_position\": image_position,\n",
    "        \"content_position\": content_position\n",
    "    })\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25722be9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
